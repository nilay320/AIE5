{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckbbj5diaHkg"
      },
      "source": [
        "# Fine-tuning Embeddings for RAG on Specific Data\n",
        "\n",
        "As we start our \"fine-tuning\" week, we'll start with the lowest hanging improvement one can do for RAG - which is:\n",
        "\n",
        "Fine-tuning embeddings!\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  - Task 1: Dependencies and Boilerplate\n",
        "  - Task 2: Loading Data\n",
        "  - Task 3: Constructing a Fine-tuning Dataset\n",
        "  - Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
        "  - Task 5: Evaluating our Retriever\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwor_3X6ODX"
      },
      "source": [
        "#### Basic Overview of Fine-tuning Embeddings\n",
        "\n",
        "In essence, what we want to do when we fine-tune our embedding models is very simple:\n",
        "\n",
        "```\n",
        "Move the embeddings for questions relating to a document\n",
        "closer together with that document\n",
        "```\n",
        "\n",
        "We can think of fine-tuning our embedding models as follows:\n",
        "\n",
        "1) We have some pair of text items that *should* be closer together\n",
        "  - `Question`, `Document` pairs\n",
        "  - EX: `Who drives the bus?`, `The bus was driven by Kyle, the Bus Driver`.\n",
        "\n",
        "2) We use these pairs as labeled data to fine-tune our embedding model.\n",
        "\n",
        "The process of training helps the model more accurately associate our questions with the correct documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX5R3HVz6FOQ"
      },
      "source": [
        "#####‚ùì Question #1:\n",
        "\n",
        "Describe the nuance between using Q&D pairs to train the embedding model vs. inter-document pairs/related sentences.\n",
        "\n",
        "What caveats does this approach have? Are there any special considerations for what kind of Q's we should use?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "We are specifically relating *the questions* to *the documents*. This means that we are making our embedding model at the very specific task of relating potential questions to specific documents.\n",
        "\n",
        "There are many caveats, but the main ones are:\n",
        "\n",
        "- Your Q's should reflect the Q's of your users\n",
        "- This kind of fine-tuning will (purposefully) \"overfit\" on your data; this is the desired result in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just to add to this:\n",
        "\n",
        "Q&D training specifically optimizes for retrieval tasks where users ask questions, while inter-document or related sentence training optimizes for document-to-document similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NkSaurzbpyS"
      },
      "source": [
        "## Task 1: Dependencies and Boilerplate\n",
        "\n",
        "We'll set up our `nest_asyncio` so we can leverage async loops in our Notebook.\n",
        "\n",
        "We'll also install the required libraries we'll be using today, and set up our OpenAI API key!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_EUibmcDU3"
      },
      "source": [
        "### Nest Asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zq-6s7LbPnKH"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8uFz8RVcFFu"
      },
      "source": [
        "### Install Dependencies\n",
        "\n",
        ">> NOTE: You do not need to do these steps if you are running this notebook locally with `uv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulZIBA1ZoSsV",
        "outputId": "12d9c766-843f-40bf-bdf8-e0ed04b6d87f"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain_openai langchain_huggingface langchain_core langchain langchain_community langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3GFD7B-tOCrx"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU faiss-cpu python-pptx==1.0.2 nltk==3.9.1 pymupdf beautifulsoup4 lxml "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FM-eUlrcI8a"
      },
      "source": [
        "### Provide OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA_mlurVqtrp",
        "outputId": "18cccb1e-095f-40fa-def5-2454f9bcdcae"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFZ217gCDVTr"
      },
      "source": [
        "## Task 2: Loading Data\n",
        "\n",
        "We'll prepare our data - and download our webpages which we'll be using for our data today.\n",
        "\n",
        "These webpages are from [Simon Willison's](https://simonwillison.net/) yearly \"AI learnings\".\n",
        "\n",
        "- [2023 Blog](https://simonwillison.net/2023/Dec/31/ai-in-2023/)\n",
        "- [2024 Blog](https://simonwillison.net/2024/Dec/31/llms-in-2024/)\n",
        "\n",
        "Let's start by collecting our data into a useful pile!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "!curl https://simonwillison.net/2023/Dec/31/ai-in-2023/ -o data/2023_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "!curl https://simonwillison.net/2024/Dec/31/llms-in-2024/ -o data/2024_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DHJhTzsvN75t"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "\n",
        "path = \"data/\"\n",
        "text_loader = DirectoryLoader(path, glob=\"*.html\", loader_cls=BSHTMLLoader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UbKa6-V0nvp"
      },
      "source": [
        "Next, we'll set up a classic naive chunking strategy as we only care that the documents get parsed into chunks that we can generate synthetic questions about."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NsPrOOqXOsNX"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf_PoX7l09Rg"
      },
      "source": [
        "Next we can load/split these documents as follows.\n",
        "\n",
        ">> NOTE: You may need to run this cell twice to get it to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OMYPX6N6Os8M"
      },
      "outputs": [],
      "source": [
        "training_documents = text_splitter.split_documents(text_loader.load())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAozuMoNOvnp",
        "outputId": "dc1d663e-7153-4c51-cedb-d1bc3888c4ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(training_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yE2TFIq1BuJ"
      },
      "source": [
        "Next, we're going to associate each of our chunks with a unique identifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "AwyIForybIpo"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "id_set = set()\n",
        "\n",
        "for document in training_documents:\n",
        "  id = str(uuid.uuid4())\n",
        "  while id in id_set:\n",
        "    id = uuid.uuid4()\n",
        "  id_set.add(id)\n",
        "  document.metadata[\"id\"] = id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJnL4oNg341U"
      },
      "source": [
        "Next, we'll simply use naive Python slicing to create a training, test, and validation set to prepare our data for the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MTS4GTSEcnG4"
      },
      "outputs": [],
      "source": [
        "training_split_documents = training_documents[:len(training_documents) - 24]\n",
        "val_split_documents = training_documents[len(training_documents) - 24:102-12]\n",
        "test_split_documents = training_documents[102-12:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzlvKbONDWvQ"
      },
      "source": [
        "## Task 3: Constructing a Fine-tuning Dataset\n",
        "\n",
        "Using the nodes we created above, we can finally start constructing a fine-tuning dataset utilizing OpenAI's `gpt-4o-mini` (released [today](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)).\n",
        "\n",
        "The basic idea here is straightforward enough:\n",
        "\n",
        "1. We look at a document\n",
        "2. We generate questions that could be answered by that node\n",
        "\n",
        "This gives us a number of question/context pairs that we can use to fine-tune our Embeddings model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_EWfmIscMrvg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "qa_chat_model = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-hLnsSB6Y-S"
      },
      "source": [
        "We'll create a simple Question Generation prompt to query `gpt-4o-mini` to generate Questions for each retrieved context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "diEWcw00NMSj"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "qa_prompt = \"\"\"\\\n",
        "Given the following context, you must generate questions based on only the provided context.\n",
        "\n",
        "You are to generate {n_questions} questions which should be provided in the following format:\n",
        "\n",
        "1. QUESTION #1\n",
        "2. QUESTION #2\n",
        "...\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u87Izpgm6_fk"
      },
      "source": [
        "We'll create a simple chain to query the LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ggl9SSjiNbpG"
      },
      "outputs": [],
      "source": [
        "question_generation_chain = qa_prompt_template | qa_chat_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4duvHirh7DQv"
      },
      "source": [
        "There's a lot going on in this function - let's take a deeper look:\n",
        "\n",
        "1. First, we provide a list of documents and a number of questions\n",
        "2. We, for each document in our list, generate `n_questions` of questions.\n",
        "3. We then associate those questions and contexts via a `UUID`.\n",
        "\n",
        "> NOTE: The reason we're doing this `UUID` association is for ease of use later in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lm2JvgC9X37"
      },
      "source": [
        "##### üèóÔ∏è Activity #1:\n",
        "\n",
        "We have:\n",
        "\n",
        "- Lists of `Documents` with the `metadata` field `id`.\n",
        "\n",
        "We need:\n",
        "\n",
        "- An object with key `id`, which have values `str` questions.\n",
        "- An object with key `question_id`, which have values `List(str)` which will be a list of associated `context_id`.\n",
        "\n",
        "An Example:\n",
        "\n",
        "question_object:\n",
        "```python\n",
        "{\n",
        "'b4b95fb6-f827-4454-aa5b-20e62733f172': 'What types of accessible formats are available for persons with disabilities?',\n",
        "'df58ee4f-714c-419e-8324-94e5870574e2': 'How do accessible formats benefit persons with disabilities?',\n",
        "'505fce8b-0e56-48de-a251-61027e396918': 'What are some of the risks associated with the increasing capabilities of AI systems that generate synthetic content?',\n",
        "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': 'Why is it important for providers of AI systems to embed technical solutions for marking and detecting synthetic content?'\n",
        "}\n",
        " ```\n",
        "\n",
        " context_object:\n",
        " ```python\n",
        "{\n",
        "'b4b95fb6-f827-4454-aa5b-20e62733f172': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
        "'df58ee4f-714c-419e-8324-94e5870574e2': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
        "'505fce8b-0e56-48de-a251-61027e396918': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
        "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
        "}\n",
        " ```\n",
        "\n",
        " As you can see, a piece of context can be associated with more than 1 question.\n",
        "\n",
        " The task is to write the Python function(s) to accomplish this task.\n",
        "\n",
        " Your function signature is provided below, along with the desired return values.\n",
        "\n",
        " > NOTE: You can make any modifications that you desire - assuming that you have the correct input and outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "U4yi4NfTCnLc"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import uuid\n",
        "from tqdm import tqdm\n",
        "\n",
        "async def process_document(document, n_questions):\n",
        "    questions_generated = await question_generation_chain.ainvoke({\"context\": document.page_content, \"n_questions\": n_questions})\n",
        "\n",
        "    doc_questions = {}\n",
        "    doc_relevant_docs = {}\n",
        "\n",
        "    for question in questions_generated.content.split(\"\\n\"):\n",
        "        question_id = str(uuid.uuid4())\n",
        "        doc_questions[question_id] = \"\".join(question.split(\".\")[1:]).strip()\n",
        "        doc_relevant_docs[question_id] = [document.metadata[\"id\"]]\n",
        "\n",
        "    return doc_questions, doc_relevant_docs\n",
        "\n",
        "async def create_questions(documents, n_questions):\n",
        "    tasks = [process_document(doc, n_questions) for doc in documents]\n",
        "\n",
        "    questions = {}\n",
        "    relevant_docs = {}\n",
        "\n",
        "    for task in tqdm(asyncio.as_completed(tasks), total=len(documents), desc=\"Processing documents\"):\n",
        "        doc_questions, doc_relevant_docs = await task\n",
        "        questions.update(doc_questions)\n",
        "        relevant_docs.update(doc_relevant_docs)\n",
        "\n",
        "    return questions, relevant_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W0eWOUo4QGL"
      },
      "source": [
        "### REMOVE `await` IF NOT USING ASYNC (HINT: Use `async`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85Dq6KRqEs0F",
        "outputId": "60dcd580-b6e8-4e3b-d605-05b492ca5c96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:05<00:00, 14.13it/s]\n"
          ]
        }
      ],
      "source": [
        "training_questions, training_relevant_contexts = await create_questions(training_split_documents, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FSTG0bb7w73"
      },
      "source": [
        "We'll use the function to generate training, validation, and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIZm4CqGVzBx",
        "outputId": "65a7703a-c528-40f6-aff4-1be84902cfc4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:01<00:00,  9.49it/s]\n"
          ]
        }
      ],
      "source": [
        "val_questions, val_relevant_contexts = await create_questions(val_split_documents, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6qUHg9sV2_y",
        "outputId": "b03bf5c6-d392-40bf-a061-1daceba2962e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:02<00:00,  5.36it/s]\n"
          ]
        }
      ],
      "source": [
        "test_questions, test_relevant_contexts = await create_questions(test_split_documents, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_jYOnAI43zK"
      },
      "source": [
        "### Reformating and Saving Datasets\n",
        "\n",
        "Now, we can save our datasets for later use!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "iF6IFFq9VsNu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
        "\n",
        "train_dataset = {\n",
        "    \"questions\" : training_questions,\n",
        "    \"relevant_contexts\" : training_relevant_contexts,\n",
        "    \"corpus\" : training_corpus\n",
        "}\n",
        "\n",
        "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(train_dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "PqF9WaueV-V8"
      },
      "outputs": [],
      "source": [
        "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n",
        "\n",
        "val_dataset = {\n",
        "    \"questions\" : val_questions,\n",
        "    \"relevant_contexts\" : val_relevant_contexts,\n",
        "    \"corpus\" : val_corpus\n",
        "}\n",
        "\n",
        "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(val_dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0DSQ7WMnWAu6"
      },
      "outputs": [],
      "source": [
        "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
        "\n",
        "test_dataset = {\n",
        "    \"questions\" : test_questions,\n",
        "    \"relevant_contexts\" : test_relevant_contexts,\n",
        "    \"corpus\" : train_corpus\n",
        "}\n",
        "\n",
        "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(test_dataset, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAwklqQCgVi-"
      },
      "source": [
        "## Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
        "\n",
        "Now that we have a dataset, let's grab a `sentence-transformers` Embeddings model!\n",
        "\n",
        "We'll be using Snowflake's [`snowflake-arctic-embed-l`](https://huggingface.co/Snowflake/snowflake-arctic-embed-l) as a base embeddings model.\n",
        "\n",
        "It is a well performing embeddings model by itself, but there's a lot of very specific domain terms and vocabulary in our courpus - so lets fine-tune it and see what that can do for us!\n",
        "\n",
        ">> NOTE: Skip installing dependencies if you are running this notebook locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXzVHP3v1Cno",
        "outputId": "a9d6ca65-d355-460d-de89-7446a441512b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "pylibcudf-cu12 24.12.0 requires pyarrow<19.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 19.0.0 which is incompatible.\n",
            "cudf-cu12 24.12.0 requires pyarrow<19.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 19.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#!pip install -qU sentence_transformers datasets pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-PGsQB7Xo6V",
        "outputId": "8df58392-a82b-45f9-ce4e-b4155522e2c6"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_id = \"Snowflake/snowflake-arctic-embed-l\"\n",
        "model = SentenceTransformer(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ztG07iB8CFO"
      },
      "source": [
        "We'll grab some necessary imports from `sentence_transformers` and `torch`.\n",
        "\n",
        "> NOTE: PyTorch (`torch`) is a popular machine learning library - while we don't go very deep into PyTorch it's an incredibly powerful and interesting library! Please read more about it [here](https://pytorch.org/tutorials/beginner/basics/intro.html)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "B-WbpuUWYFJr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from sentence_transformers import InputExample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJtPPlck8HBE"
      },
      "source": [
        "We're using a toy batch size here to reflect the limited number of examples we have.\n",
        "\n",
        "> NOTE: It is typical to use a much larger batch size (~64+), hardware permitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8Lokhy6KYHAv"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-6DT8hc8PmT"
      },
      "source": [
        "Let's move our dataset into the expected format for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "JJk37zQsYJ4P"
      },
      "outputs": [],
      "source": [
        "corpus = train_dataset['corpus']\n",
        "queries = train_dataset['questions']\n",
        "relevant_docs = train_dataset['relevant_contexts']\n",
        "\n",
        "examples = []\n",
        "for query_id, query in queries.items():\n",
        "    doc_id = relevant_docs[query_id][0]\n",
        "    text = corpus[doc_id]\n",
        "    example = InputExample(texts=[query, text])\n",
        "    examples.append(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjFx7KHI8TL0"
      },
      "source": [
        "Now we can create a `torch` `DataLoader`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "tiizmeIqZ_-w"
      },
      "outputs": [],
      "source": [
        "loader = DataLoader(\n",
        "    examples, batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vA8rzlX8XbT"
      },
      "source": [
        "Next up, we'll prepare our loss function!\n",
        "\n",
        "Loss is an important part of training, fine-tuning, and more. If you want a deep dive on loss - you can check out our [event on loss!](https://www.youtube.com/watch?v=iB8FWR9aD5Q&t=8s).\n",
        "\n",
        "The core loss we're using today is called `MultipleNegativesRankingLoss` - you can find more information [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py).\n",
        "\n",
        "This is \"wrapped\" in `MatryoshkaLoss`, which you can read the implementation of [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MatryoshkaLoss.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Uga4nnBqlVeh"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
        "\n",
        "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
        "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
        "train_loss = MatryoshkaLoss(\n",
        "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJG4fOm66PHI"
      },
      "source": [
        "##### üèóÔ∏è Activity #2:\n",
        "\n",
        "Both of these losses sound \"cool\", but what are they - exactly - under the hood?\n",
        "\n",
        "Why are these losses specifically doing? Please write a short summary of each loss.\n",
        "\n",
        "> NOTE: This is a course focused on AI Engineering and the application of AI - looking for a hint? Try pasting the code (linked above) into ChatGPT/Claude to write the summary!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer\n",
        "\n",
        "MultipleNegativesRankingLoss\n",
        "This loss function is designed specifically for contrastive learning in sentence embeddings, which is ideal for our retrieval task. Here's what it does:\n",
        "\n",
        "It takes pairs of texts (anchors and positives) that should be close to each other in the embedding space\n",
        "For each anchor, it treats all other texts in the batch as negative examples\n",
        "The loss function maximizes the similarity between anchor-positive pairs while minimizing similarity to all other texts in the batch\n",
        "This effectively creates a \"pull\" between related texts and a \"push\" away from unrelated ones\n",
        "The large number of negative examples per positive pair makes training more efficient\n",
        "\n",
        "In our case, this helps ensure that questions become close to their relevant documents and far from irrelevant ones.\n",
        "MatryoshkaLoss\n",
        "MatryoshkaLoss is a meta-loss that enables embedding models to create representations at multiple dimensions simultaneously:\n",
        "\n",
        "It wraps around another loss function (in this case MultipleNegativesRankingLoss)\n",
        "It applies the loss at multiple embedding dimensions (768, 512, 256, 128, 64 in our example)\n",
        "This creates \"nested\" embeddings where smaller dimensions preserve the important information from larger ones\n",
        "The model learns to compress information efficiently at each dimension\n",
        "This allows for flexibility in deployment - we can choose smaller embedding sizes when speed is critical or larger ones when accuracy matters more\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKxRuXfH844c"
      },
      "source": [
        "Now we can set-up our evaluator.\n",
        "\n",
        "> NOTE: Due to the formatting of our dataset - this is all we have to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "f0hAFwUyaHQG"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "\n",
        "corpus = val_dataset['corpus']\n",
        "queries = val_dataset['questions']\n",
        "relevant_docs = val_dataset['relevant_contexts']\n",
        "\n",
        "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYfap_ct8-bU"
      },
      "source": [
        "We'll train this model for 5 epochs, though you could increase this number if we had a significant amount more data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "svZG0pBHiQr6"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxitWoNX9DwW"
      },
      "source": [
        "It's training time!\n",
        "\n",
        "> NOTE: We're manually defining a warm-up period here - this is just to provide a smooth ramp into our training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/ssjyoobf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x351650ec0>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.init(mode=\"disabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753,
          "referenced_widgets": [
            "0d3fc6edfdab4fe9aff8e805632eadad",
            "ebe8aa7a82124b57bce2d826c1dea0fa",
            "aa11c10b4234452d9430744ab89b59a2",
            "ca05cbcd72cb41d9856804ffb17b26cb",
            "cc2a33e9a7ac4c5699346fcbf53b7c95",
            "408f4dfce21a45cfad36047677ec8658",
            "ca5804644ef345c1b4f670fb7f088fe8",
            "2a872283afaa4a33a9bc3f9e57b3650b",
            "fe4d1052824c4ed29c38c5311087e650",
            "e283b1608c4d4266a03c57dc95aabb2e",
            "bfc4997e3bd94e66bebaa1ffdae1b99e"
          ]
        },
        "id": "aDhUHZY-iR09",
        "outputId": "6dbd9320-f7b9-46d6-b891-efdd12086631"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9dc48cbba596412ebed7b57e2656f32d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/160 02:05, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Cosine Accuracy@1</th>\n",
              "      <th>Cosine Accuracy@3</th>\n",
              "      <th>Cosine Accuracy@5</th>\n",
              "      <th>Cosine Accuracy@10</th>\n",
              "      <th>Cosine Precision@1</th>\n",
              "      <th>Cosine Precision@3</th>\n",
              "      <th>Cosine Precision@5</th>\n",
              "      <th>Cosine Precision@10</th>\n",
              "      <th>Cosine Recall@1</th>\n",
              "      <th>Cosine Recall@3</th>\n",
              "      <th>Cosine Recall@5</th>\n",
              "      <th>Cosine Recall@10</th>\n",
              "      <th>Cosine Ndcg@10</th>\n",
              "      <th>Cosine Mrr@10</th>\n",
              "      <th>Cosine Map@100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969244</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
        "\n",
        "model.fit(\n",
        "    train_objectives=[(loader, train_loss)],\n",
        "    epochs=EPOCHS,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path='finetuned_arctic_ft',\n",
        "    show_progress_bar=True,\n",
        "    evaluator=evaluator,\n",
        "    evaluation_steps=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "58699484312f460cb96ac44e3af14aa2",
            "d45a7d95e5b14a6f88d5798fec9c40a7",
            "b011a6ccf8e745c6be6f3a17b7d61dce",
            "1016780729b04ba489d488922173ddae",
            "9d34dac405fd40139d5dc04eecef55ed",
            "d6169577ffb341a69eb0175e301b6a44",
            "d4acc9a7aca04564bfaefe33de079394",
            "4e2c257232c3472697e03350de43cb30",
            "381c780ce17e4662981175400fb8a0f8",
            "878dc96f87dd45f389948a546db33e94",
            "006bf6f5ebaf400486a6b82610381db0",
            "9198dd0fa8f04aa7b75307aa2d513bfb",
            "59cd26ae53024fbd85431b6683cd119c",
            "4e7ccd97042c4fb9a201b6dc76762e04",
            "72ec09e2cbbf4788b1561abfcdd0819a",
            "fb1e19624fde4d3c8048ce00264d6056",
            "9638a0456e0c41b1b9b9757932f55c53",
            "18825dc83221412ab602830fc00db71b",
            "a6ea48a80c194d128959422368aa0e10",
            "9f4026c62c60493caa18c014ae414e65"
          ]
        },
        "id": "b3iwclvyRD8L",
        "outputId": "1471e984-9351-478c-de34-6eb32263fa30"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb0719b52f514f39acb12fc39e3dcfaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_pn-Y6yjRoHk"
      },
      "outputs": [],
      "source": [
        "hf_username = \"llm-wizard\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "e5c86e0e33264ed8b6325e44f9421653",
            "acf3991e5d644f468264f561b28b514a",
            "4a723c6a56e94309b2e3abe63f28aedc",
            "ef25768d3b0746b48c6d02e9bd151bf9",
            "cc1f0bc4a1a74568b9c74a7392a1568f",
            "95160a05de5b402b9bdb0bd2d099cd00",
            "cf376b0ea3544055b8867d7013526502",
            "869814ecd49e46f9a33dd53130e6953f",
            "cc7d460d3c5a4ac6a9b64929736d6598",
            "13e9bf583f6442d395334947379a280a",
            "1995b4d98fe044e38f1980d47033ca40"
          ]
        },
        "id": "Nqhf3zWa9AiJ",
        "outputId": "c601b2a8-f8e9-4d71-9c7b-8ea4999ff077"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5333c128ca9941ffa8e76716abfdead3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/njhaveri/legal-ft-2/commit/80915d9c5597708eddd7b73e5b610e2f0cac6914'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.push_to_hub(f\"njhaveri/legal-ft-2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bo0zW5k9Poq"
      },
      "source": [
        "## Task 5: Evaluating our Retriever\n",
        "\n",
        "Now that we have fine-tuned our retriever - let's see if it's worthwhile!\n",
        "\n",
        "We'll start with some basic imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Vq-2oqU0wHFr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jD0qrIh9X8f"
      },
      "source": [
        "Now we'll define a function that will help us evaluate our retrieval process.\n",
        "\n",
        "> NOTE: We're assuming 1 correct document in a \"hit\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0713_3cowX4q"
      },
      "outputs": [],
      "source": [
        "def evaluate_openai(\n",
        "    dataset,\n",
        "    embed_model,\n",
        "    top_k=5,\n",
        "    verbose=False,\n",
        "):\n",
        "  corpus = dataset['corpus']\n",
        "  questions = dataset['questions']\n",
        "  relevant_docs = dataset['relevant_contexts']\n",
        "  documents = [Document(page_content=content, metadata={\"id\": doc_id}) for doc_id, content in corpus.items()]\n",
        "  vectorstore = FAISS.from_documents(documents, embed_model)\n",
        "\n",
        "  retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
        "\n",
        "  eval_results = []\n",
        "  for id, question in tqdm(questions.items()):\n",
        "    retrieved_nodes = retriever.invoke(question)\n",
        "    retrieved_ids = [node.metadata[\"id\"] for node in retrieved_nodes]\n",
        "    expected_id = relevant_docs[id][0]\n",
        "    is_hit = expected_id in retrieved_ids\n",
        "    eval_results.append({\"id\": id, \"question\": question, \"expected_id\": expected_id, \"is_hit\": is_hit})\n",
        "\n",
        "  return eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOr49m4O9lxY"
      },
      "source": [
        "All that's left to do is evaluate, we'll evaluate our model against:\n",
        "\n",
        "1. OpenAI's closed source `text-embedding-3-small`\n",
        "2. The base non-fine-tuned version of `Snowflake/snowflake-arctic-embed-l`.\n",
        "\n",
        "Let's see how it stacks up!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijaeYpf593IW"
      },
      "source": [
        "### `text-embedding-3-small`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyY3PztaxnU3",
        "outputId": "5a1ec5e9-00fc-4140-d5b6-aa752dd4c9fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:08<00:00,  2.95it/s]\n"
          ]
        }
      ],
      "source": [
        "te3_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "te3_results = evaluate_openai(test_dataset, te3_openai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "kkyW90TCxx_i"
      },
      "outputs": [],
      "source": [
        "te3_results_df = pd.DataFrame(te3_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MscVRdNCylJ-",
        "outputId": "275beff8-3c59-4063-8270-c01736b4ee05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(1.0)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n",
        "te3_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ra-mh0L96dQ"
      },
      "source": [
        "### `Snowflake/snowflake-arctic-embed-l` (base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEskxwvFypHe",
        "outputId": "a3aad8ce-48ef-4d8f-9ed0-122b1ec9a000"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:05<00:00,  4.22it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\")\n",
        "arctic_embed_m_results = evaluate_openai(test_dataset, huggingface_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "KlKgiXTWzMTg"
      },
      "outputs": [],
      "source": [
        "arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV5vJWrJzOhc",
        "outputId": "30049be6-deb9-4ceb-dc13-24d7e125f131"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.9166666666666666)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n",
        "arctic_embed_m_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcR3-0s19_lu"
      },
      "source": [
        "### `Snowflake/snowflake-arctic-embed-l` (fine-tuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilse1LduzP1i",
        "outputId": "292ab58f-7594-45fc-a5b8-1062cc553f66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at finetuned_arctic_ft and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:01<00:00, 23.85it/s]\n"
          ]
        }
      ],
      "source": [
        "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"finetuned_arctic_ft\")\n",
        "finetune_results = evaluate_openai(test_dataset, finetune_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "xxhZPqkNzZlh"
      },
      "outputs": [],
      "source": [
        "finetune_results_df = pd.DataFrame(finetune_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4thAK2BXzaj6",
        "outputId": "e890b5d1-86b7-4bfe-8ffe-a779a132e0c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(1.0)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_hit_rate = finetune_results_df[\"is_hit\"].mean()\n",
        "finetune_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iegFM209mBk3"
      },
      "source": [
        "## Task 1: Vibe Checking the RAG Pipeline\n",
        "\n",
        "We're going to use our RAG pipeline to vibe check on some common phrases now that we've modified it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzg0AA5krgR4"
      },
      "source": [
        "### Creating New Chunks\n",
        "\n",
        "In order to try and evaluate our system more fairly, let's create new chunks that we will use to create our Vector Store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "KwQ2_LqNr0Tw"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 600,\n",
        "    chunk_overlap  = 50,\n",
        "    length_function = len\n",
        ")\n",
        "\n",
        "training_documents = text_splitter.split_documents(text_loader.load())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIdxahHXpP-c"
      },
      "source": [
        "### Base Chain\n",
        "\n",
        "We'll start by constructing our base chain, which will use the untrained retrieval model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOsxIXpNpWC2"
      },
      "source": [
        "#### R - Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "azIGIKYfmNCT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "base_vectorstore = FAISS.from_documents(training_documents, huggingface_embeddings)\n",
        "base_retriever = base_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-1nVZ0KpX5N"
      },
      "source": [
        "#### A - Augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "G10Fr-aKojeA"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and a question, you must answer the question. If you do not know the answer, you must state that you do not know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euq6RQEopZvD"
      },
      "source": [
        "#### G - Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "5-mfbbrypMHG"
      },
      "outputs": [],
      "source": [
        "rag_llm =  ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ2p4mnUpbYY"
      },
      "source": [
        "#### RAG - LCEL RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "ssuR-LaboyGq"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "\n",
        "base_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "emm6WbB9pfKt",
        "outputId": "f0e0c83f-c617-493e-99ce-869061a315f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'An agent, in the context of AI, is a term that refers to AI systems that can act on your behalf. However, the term is vague and lacks a single, clear definition, leading to various interpretations. Some people view agents as systems that autonomously perform tasks, while others see them as LLMs (large language models) equipped with tools to solve problems. The concept is still evolving, and there is skepticism about their utility due to challenges such as gullibility, where these systems may struggle to distinguish truth from fiction.'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is an agent?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "mUOrd0OBprAq",
        "outputId": "0070d677-0bde-48be-a8a3-f0c53b9eee90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Better-than-GPT-3 class models have been produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu, and several other organizations.'"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OnfuFl59py7I",
        "outputId": "37480e29-17a6-40e2-fe8a-7eb6ea270569"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I do not know.'"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is the laziest AI month?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "-NmqwHBDqTZ8",
        "outputId": "86d7b59a-97c6-4b65-805f-ae449e3b1a20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I do not know.'"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqNS0UJAp3lC"
      },
      "source": [
        "### Fine-tuned Embedding Model\n",
        "\n",
        "Now let's rebuild our RAG chain with the Fine-tuned model - the only component we need to change is our `FAISS` vectorstore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "ihO7tP6mqATy"
      },
      "outputs": [],
      "source": [
        "finetune_vectorstore = FAISS.from_documents(training_documents, finetune_embeddings)\n",
        "finetune_retriever = finetune_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "1_cIFvWzqKGY"
      },
      "outputs": [],
      "source": [
        "finetune_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | finetune_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OJmRHJF2qNgj",
        "outputId": "b99d4b58-8487-48ec-ee6c-8ea93f9b0192"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'An \"agent\" is a term that lacks a single, clear, and widely understood meaning in the context of AI. It is often used to refer to AI systems that can act on behalf of a user, but there are various interpretations of what this entails. Some people view agents as systems that autonomously perform tasks, while others think of them as LLMs (Large Language Models) that utilize tools to solve problems. The term is often associated with concepts like autonomy, but there is significant ambiguity surrounding its definition and practical implementation. Overall, the concept of agents remains largely theoretical, with few examples in production and ongoing challenges related to issues like gullibility.'"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is an Agent?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "EnK-c2ugqPPh",
        "outputId": "b8300e0a-1b51-48dd-93c3-254e0aa84e36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Organizations that have produced better-than-GPT-3 class models include Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu, and several others.'"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "83hssg1AWozc",
        "outputId": "8d1ef134-7889-4379-a49b-6c0a476b7a1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I do not know.'"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is the laziest AI month?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "rsHmGeFbqRET",
        "outputId": "1ab223c2-d2fc-46ac-8541-af038de3166d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The largest model that Simon has run on his phone is the Llama 3.2 3B model.'"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDgD8seY_I3W"
      },
      "source": [
        "####‚ùìQuestion #2:\n",
        "\n",
        "Which LCEL RAG Chain do you think answered the questions better, and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Answer\n",
        "The fine-tuned LCEL RAG chain typically responds better than the base chain. After training on a dataset of question‚Äìdocument pairs, the embeddings are better aligned with your specific content, so queries map more directly to relevant chunks. This results in a higher ‚Äúhit rate‚Äù during retrieval (i.e., the correct passage is found more often) and more accurate responses‚Äîparticularly for domain-specific questions like ‚ÄúWhat is the largest model Simon has run on his phone?‚Äù The base chain, on the other hand, lacks that specialized training and therefore does not retrieve or answer as accurately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCbq1sZArIx4"
      },
      "source": [
        "## Task 2: RAGAS Evaluation\n",
        "\n",
        "It's great to have some idea of how our system is doing based on vibe-checks, but let's use RAGAS to provide more insight info. on how things are improving!\n",
        "\n",
        "> NOTE: Please recreate *exactly* the RAGAS process we used to evaluate RAG, baselining with the default retriever, and then comparing the new retriever. The includes the Synthetic Data Generation steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq880DtHk9pX"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "os.environ[\"RAGAS_APP_TOKEN\"] = getpass(\"Please enter your Ragas API key!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nilayjhaveri/Workspace/Projects/ai_maker_space/ai_engineering/code/AIE5/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/pysbd/segmenter.py:66: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  for match in re.finditer('{0}\\s*'.format(re.escape(sent)), self.original_text):\n",
            "/Users/nilayjhaveri/Workspace/Projects/ai_maker_space/ai_engineering/code/AIE5/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/pysbd/lang/arabic.py:29: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  txt = re.sub('(?<={0})\\.'.format(am), '‚àØ', txt)\n",
            "/Users/nilayjhaveri/Workspace/Projects/ai_maker_space/ai_engineering/code/AIE5/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/pysbd/lang/persian.py:29: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  txt = re.sub('(?<={0})\\.'.format(am), '‚àØ', txt)\n"
          ]
        }
      ],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = text_loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18d72ad4524c44a7baa47a270e7ae970",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc36369738f44c95a82c1be0c7d01e2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9b6f3e472854b52ab216d133e213e40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "958b3df6bc1d4e13a8df17b11fa93135",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8adfd4e62b9a4e25bc518c2a2d97f590",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/26 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90d535683ee94c1b84df5682e762c0f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0539b0a3efe43fdbe20a3c294aae968",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cc77530a3844a818b733f6e932fec4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8d16de5c1a94e989233bafc861c6dd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://app.ragas.io/dashboard/alignment/testset/6f0d9656-2f96-40a8-8fde-3c38b895ab49'"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = base_rag_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"]\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What role does Replit play in the development ...</td>\n",
              "      <td>[Prompt injection explained, with video, slide...</td>\n",
              "      <td>[Code may be the best application The ethics o...</td>\n",
              "      <td>I do not know.</td>\n",
              "      <td>Replit is one of the organizations that have p...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What significant event related to prompt injec...</td>\n",
              "      <td>[Prompt injection explained, with video, slide...</td>\n",
              "      <td>[Based Development As a computer scientist and...</td>\n",
              "      <td>I do not know.</td>\n",
              "      <td>In September last year, the term 'prompt injec...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What were the significant developments in AI, ...</td>\n",
              "      <td>[Prompt injection explained, with video, slide...</td>\n",
              "      <td>[Stuff we figured out about AI in 2023 Simon W...</td>\n",
              "      <td>In 2023, significant developments in AI, parti...</td>\n",
              "      <td>In 2023, Large Language Models (LLMs) were rec...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How has the development and deployment of AI, ...</td>\n",
              "      <td>[The legal arguments here are complex. I‚Äôm not...</td>\n",
              "      <td>[easy to follow. The rest of the document incl...</td>\n",
              "      <td>The development and deployment of AI, particul...</td>\n",
              "      <td>The development and deployment of AI, especial...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How has the environmental impact of AI evolved...</td>\n",
              "      <td>[The legal arguments here are complex. I‚Äôm not...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nVoice and live camera mode are sci...</td>\n",
              "      <td>The environmental impact of AI has evolved pos...</td>\n",
              "      <td>The environmental impact of AI has seen both i...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How did the shift from universal access to sub...</td>\n",
              "      <td>[For a few short months this year all three of...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nVoice and live camera mode are sci...</td>\n",
              "      <td>The shift from universal access to subscriptio...</td>\n",
              "      <td>For a few short months, the best available mod...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How has the environmental impact of AI, partic...</td>\n",
              "      <td>[I get it. There are plenty of reasons to disl...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nVoice and live camera mode are sci...</td>\n",
              "      <td>The environmental impact of AI, particularly i...</td>\n",
              "      <td>The environmental impact of AI, especially con...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How has the environmental impact of AI evolved...</td>\n",
              "      <td>[The legal arguments here are complex. I‚Äôm not...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nVoice and live camera mode are sci...</td>\n",
              "      <td>The context provided does not specifically add...</td>\n",
              "      <td>The environmental impact of AI has seen both i...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How has the development of models like DeepSee...</td>\n",
              "      <td>[Is this infrastructure necessary? DeepSeek v3...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nVoice and live camera mode are sci...</td>\n",
              "      <td>The development of models like DeepSeek v3, wh...</td>\n",
              "      <td>The development of models like DeepSeek v3, wh...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How has Anthropic's Claude 3.5 Sonnet contribu...</td>\n",
              "      <td>[Anthropic kicked this idea into high gear whe...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\npelicans: Your browser does not su...</td>\n",
              "      <td>Anthropic's Claude 3.5 Sonnet has significantl...</td>\n",
              "      <td>Anthropic's Claude 3.5 Sonnet has significantl...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What are the ethical and operational challenge...</td>\n",
              "      <td>[I get it. There are plenty of reasons to disl...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThings we learned about LLMs in 20...</td>\n",
              "      <td>In 2024, LLMs face several ethical and operati...</td>\n",
              "      <td>In 2024, Large Language Models (LLMs) faced si...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>How has Meta's release of Llama and Llama 2 in...</td>\n",
              "      <td>[Another common technique is to use larger mod...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The release of Meta's Llama and Llama 2 models...</td>\n",
              "      <td>Meta's release of Llama and Llama 2 has signif...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0   What role does Replit play in the development ...   \n",
              "1   What significant event related to prompt injec...   \n",
              "2   What were the significant developments in AI, ...   \n",
              "3   How has the development and deployment of AI, ...   \n",
              "4   How has the environmental impact of AI evolved...   \n",
              "5   How did the shift from universal access to sub...   \n",
              "6   How has the environmental impact of AI, partic...   \n",
              "7   How has the environmental impact of AI evolved...   \n",
              "8   How has the development of models like DeepSee...   \n",
              "9   How has Anthropic's Claude 3.5 Sonnet contribu...   \n",
              "10  What are the ethical and operational challenge...   \n",
              "11  How has Meta's release of Llama and Llama 2 in...   \n",
              "\n",
              "                                   retrieved_contexts  \\\n",
              "0   [Prompt injection explained, with video, slide...   \n",
              "1   [Prompt injection explained, with video, slide...   \n",
              "2   [Prompt injection explained, with video, slide...   \n",
              "3   [The legal arguments here are complex. I‚Äôm not...   \n",
              "4   [The legal arguments here are complex. I‚Äôm not...   \n",
              "5   [For a few short months this year all three of...   \n",
              "6   [I get it. There are plenty of reasons to disl...   \n",
              "7   [The legal arguments here are complex. I‚Äôm not...   \n",
              "8   [Is this infrastructure necessary? DeepSeek v3...   \n",
              "9   [Anthropic kicked this idea into high gear whe...   \n",
              "10  [I get it. There are plenty of reasons to disl...   \n",
              "11  [Another common technique is to use larger mod...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Code may be the best application The ethics o...   \n",
              "1   [Based Development As a computer scientist and...   \n",
              "2   [Stuff we figured out about AI in 2023 Simon W...   \n",
              "3   [easy to follow. The rest of the document incl...   \n",
              "4   [<1-hop>\\n\\nVoice and live camera mode are sci...   \n",
              "5   [<1-hop>\\n\\nVoice and live camera mode are sci...   \n",
              "6   [<1-hop>\\n\\nVoice and live camera mode are sci...   \n",
              "7   [<1-hop>\\n\\nVoice and live camera mode are sci...   \n",
              "8   [<1-hop>\\n\\nVoice and live camera mode are sci...   \n",
              "9   [<1-hop>\\n\\npelicans: Your browser does not su...   \n",
              "10  [<1-hop>\\n\\nThings we learned about LLMs in 20...   \n",
              "11  [<1-hop>\\n\\nCode may be the best application T...   \n",
              "\n",
              "                                             response  \\\n",
              "0                                      I do not know.   \n",
              "1                                      I do not know.   \n",
              "2   In 2023, significant developments in AI, parti...   \n",
              "3   The development and deployment of AI, particul...   \n",
              "4   The environmental impact of AI has evolved pos...   \n",
              "5   The shift from universal access to subscriptio...   \n",
              "6   The environmental impact of AI, particularly i...   \n",
              "7   The context provided does not specifically add...   \n",
              "8   The development of models like DeepSeek v3, wh...   \n",
              "9   Anthropic's Claude 3.5 Sonnet has significantl...   \n",
              "10  In 2024, LLMs face several ethical and operati...   \n",
              "11  The release of Meta's Llama and Llama 2 models...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   Replit is one of the organizations that have p...   \n",
              "1   In September last year, the term 'prompt injec...   \n",
              "2   In 2023, Large Language Models (LLMs) were rec...   \n",
              "3   The development and deployment of AI, especial...   \n",
              "4   The environmental impact of AI has seen both i...   \n",
              "5   For a few short months, the best available mod...   \n",
              "6   The environmental impact of AI, especially con...   \n",
              "7   The environmental impact of AI has seen both i...   \n",
              "8   The development of models like DeepSeek v3, wh...   \n",
              "9   Anthropic's Claude 3.5 Sonnet has significantl...   \n",
              "10  In 2024, Large Language Models (LLMs) faced si...   \n",
              "11  Meta's release of Llama and Llama 2 has signif...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "001fcb4ccbb44a37abed5d2976e90805",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/72 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[8]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
            "Exception raised in Job[2]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
            "Exception raised in Job[13]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29140, Requested 2133. Please try again in 2.546s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[23]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29864, Requested 1465. Please try again in 2.658s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[24]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29349, Requested 1974. Please try again in 2.646s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[25]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29655, Requested 1784. Please try again in 2.878s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[19]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29426, Requested 2080. Please try again in 3.012s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[30]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29782, Requested 1853. Please try again in 3.27s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[31]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29296, Requested 1872. Please try again in 2.336s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[37]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29527, Requested 1846. Please try again in 2.746s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[36]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29257, Requested 2011. Please try again in 2.536s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[42]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29110, Requested 1971. Please try again in 2.162s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[48]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29871, Requested 1821. Please try again in 3.384s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[17]: TimeoutError()\n",
            "Exception raised in Job[29]: TimeoutError()\n",
            "Exception raised in Job[49]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29529, Requested 1799. Please try again in 2.656s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[55]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29445, Requested 1914. Please try again in 2.718s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[54]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29302, Requested 1961. Please try again in 2.526s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[41]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29719, Requested 1465. Please try again in 2.368s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[35]: TimeoutError()\n",
            "Exception raised in Job[47]: TimeoutError()\n",
            "Exception raised in Job[53]: TimeoutError()\n",
            "Exception raised in Job[59]: TimeoutError()\n",
            "Exception raised in Job[65]: TimeoutError()\n",
            "Exception raised in Job[71]: TimeoutError()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.2222, 'faithfulness': 0.6364, 'factual_correctness': 0.4390, 'answer_relevancy': 0.7206, 'context_entity_recall': 0.3007, 'noise_sensitivity_relevant': 0.5000}"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "for test_row in dataset:\n",
        "  response = finetune_rag_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"]\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "  time.sleep(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What role does Replit play in the development ...</td>\n",
              "      <td>[Prompt injection explained, with video, slide...</td>\n",
              "      <td>[Code may be the best application The ethics o...</td>\n",
              "      <td>I do not know.</td>\n",
              "      <td>Replit is one of the organizations that have p...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What significant event related to prompt injec...</td>\n",
              "      <td>[The honest answer is ‚Äúmaybe‚Äù! No-one is entir...</td>\n",
              "      <td>[Based Development As a computer scientist and...</td>\n",
              "      <td>In September of the previous year, the term \"p...</td>\n",
              "      <td>In September last year, the term 'prompt injec...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What were the significant developments in AI, ...</td>\n",
              "      <td>[Stuff we figured out about AI in 2023\\n\\n\\n\\n...</td>\n",
              "      <td>[Stuff we figured out about AI in 2023 Simon W...</td>\n",
              "      <td>In 2023, significant developments in AI, parti...</td>\n",
              "      <td>In 2023, Large Language Models (LLMs) were rec...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How has the development and deployment of AI, ...</td>\n",
              "      <td>[The legal arguments here are complex. I‚Äôm not...</td>\n",
              "      <td>[easy to follow. The rest of the document incl...</td>\n",
              "      <td>The development and deployment of AI, particul...</td>\n",
              "      <td>The development and deployment of AI, especial...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How has the environmental impact of AI evolved...</td>\n",
              "      <td>[These price drops are driven by two factors: ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nVoice and live camera mode are sci...</td>\n",
              "      <td>The environmental impact of AI has evolved in ...</td>\n",
              "      <td>The environmental impact of AI has seen both i...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How did the shift from universal access to sub...</td>\n",
              "      <td>[For a few short months this year all three of...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nVoice and live camera mode are sci...</td>\n",
              "      <td>The shift from universal access to subscriptio...</td>\n",
              "      <td>For a few short months, the best available mod...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How has the environmental impact of AI, partic...</td>\n",
              "      <td>[These price drops are driven by two factors: ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nVoice and live camera mode are sci...</td>\n",
              "      <td>The environmental impact of AI, particularly i...</td>\n",
              "      <td>The environmental impact of AI, especially con...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How has the environmental impact of AI evolved...</td>\n",
              "      <td>[Those US export regulations on GPUs to China ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nVoice and live camera mode are sci...</td>\n",
              "      <td>The environmental impact of AI, particularly w...</td>\n",
              "      <td>The environmental impact of AI has seen both i...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How has the development of models like DeepSee...</td>\n",
              "      <td>[Is this infrastructure necessary? DeepSeek v3...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nVoice and live camera mode are sci...</td>\n",
              "      <td>The development of models like DeepSeek v3 and...</td>\n",
              "      <td>The development of models like DeepSeek v3, wh...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How has Anthropic's Claude 3.5 Sonnet contribu...</td>\n",
              "      <td>[Anthropic kicked this idea into high gear whe...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\npelicans: Your browser does not su...</td>\n",
              "      <td>Anthropic's Claude 3.5 Sonnet has significantl...</td>\n",
              "      <td>Anthropic's Claude 3.5 Sonnet has significantl...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What are the ethical and operational challenge...</td>\n",
              "      <td>[The rise of inference-scaling ‚Äúreasoning‚Äù mod...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThings we learned about LLMs in 20...</td>\n",
              "      <td>The ethical and operational challenges faced b...</td>\n",
              "      <td>In 2024, Large Language Models (LLMs) faced si...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>How has Meta's release of Llama and Llama 2 in...</td>\n",
              "      <td>[The big news to end the year was the release ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>Meta's release of Llama and Llama 2 has signif...</td>\n",
              "      <td>Meta's release of Llama and Llama 2 has signif...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0   What role does Replit play in the development ...   \n",
              "1   What significant event related to prompt injec...   \n",
              "2   What were the significant developments in AI, ...   \n",
              "3   How has the development and deployment of AI, ...   \n",
              "4   How has the environmental impact of AI evolved...   \n",
              "5   How did the shift from universal access to sub...   \n",
              "6   How has the environmental impact of AI, partic...   \n",
              "7   How has the environmental impact of AI evolved...   \n",
              "8   How has the development of models like DeepSee...   \n",
              "9   How has Anthropic's Claude 3.5 Sonnet contribu...   \n",
              "10  What are the ethical and operational challenge...   \n",
              "11  How has Meta's release of Llama and Llama 2 in...   \n",
              "\n",
              "                                   retrieved_contexts  \\\n",
              "0   [Prompt injection explained, with video, slide...   \n",
              "1   [The honest answer is ‚Äúmaybe‚Äù! No-one is entir...   \n",
              "2   [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n",
              "3   [The legal arguments here are complex. I‚Äôm not...   \n",
              "4   [These price drops are driven by two factors: ...   \n",
              "5   [For a few short months this year all three of...   \n",
              "6   [These price drops are driven by two factors: ...   \n",
              "7   [Those US export regulations on GPUs to China ...   \n",
              "8   [Is this infrastructure necessary? DeepSeek v3...   \n",
              "9   [Anthropic kicked this idea into high gear whe...   \n",
              "10  [The rise of inference-scaling ‚Äúreasoning‚Äù mod...   \n",
              "11  [The big news to end the year was the release ...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Code may be the best application The ethics o...   \n",
              "1   [Based Development As a computer scientist and...   \n",
              "2   [Stuff we figured out about AI in 2023 Simon W...   \n",
              "3   [easy to follow. The rest of the document incl...   \n",
              "4   [<1-hop>\\n\\nVoice and live camera mode are sci...   \n",
              "5   [<1-hop>\\n\\nVoice and live camera mode are sci...   \n",
              "6   [<1-hop>\\n\\nVoice and live camera mode are sci...   \n",
              "7   [<1-hop>\\n\\nVoice and live camera mode are sci...   \n",
              "8   [<1-hop>\\n\\nVoice and live camera mode are sci...   \n",
              "9   [<1-hop>\\n\\npelicans: Your browser does not su...   \n",
              "10  [<1-hop>\\n\\nThings we learned about LLMs in 20...   \n",
              "11  [<1-hop>\\n\\nCode may be the best application T...   \n",
              "\n",
              "                                             response  \\\n",
              "0                                      I do not know.   \n",
              "1   In September of the previous year, the term \"p...   \n",
              "2   In 2023, significant developments in AI, parti...   \n",
              "3   The development and deployment of AI, particul...   \n",
              "4   The environmental impact of AI has evolved in ...   \n",
              "5   The shift from universal access to subscriptio...   \n",
              "6   The environmental impact of AI, particularly i...   \n",
              "7   The environmental impact of AI, particularly w...   \n",
              "8   The development of models like DeepSeek v3 and...   \n",
              "9   Anthropic's Claude 3.5 Sonnet has significantl...   \n",
              "10  The ethical and operational challenges faced b...   \n",
              "11  Meta's release of Llama and Llama 2 has signif...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   Replit is one of the organizations that have p...   \n",
              "1   In September last year, the term 'prompt injec...   \n",
              "2   In 2023, Large Language Models (LLMs) were rec...   \n",
              "3   The development and deployment of AI, especial...   \n",
              "4   The environmental impact of AI has seen both i...   \n",
              "5   For a few short months, the best available mod...   \n",
              "6   The environmental impact of AI, especially con...   \n",
              "7   The environmental impact of AI has seen both i...   \n",
              "8   The development of models like DeepSeek v3, wh...   \n",
              "9   Anthropic's Claude 3.5 Sonnet has significantl...   \n",
              "10  In 2024, Large Language Models (LLMs) faced si...   \n",
              "11  Meta's release of Llama and Llama 2 has signif...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d89299287984cd3a54a43534a59603d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/72 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[24]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29263, Requested 1891. Please try again in 2.308s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[30]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29434, Requested 1945. Please try again in 2.758s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[13]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29313, Requested 1994. Please try again in 2.614s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[25]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29320, Requested 1872. Please try again in 2.384s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[19]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29323, Requested 2155. Please try again in 2.956s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[31]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29695, Requested 2042. Please try again in 3.474s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[36]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29823, Requested 2017. Please try again in 3.68s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[17]: TimeoutError()\n",
            "Exception raised in Job[42]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29667, Requested 2057. Please try again in 3.448s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[23]: TimeoutError()\n",
            "Exception raised in Job[26]: TimeoutError()\n",
            "Exception raised in Job[29]: TimeoutError()\n",
            "Exception raised in Job[44]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29826, Requested 1534. Please try again in 2.72s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[48]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29485, Requested 1948. Please try again in 2.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[43]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29371, Requested 2026. Please try again in 2.794s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[54]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29544, Requested 1928. Please try again in 2.944s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[35]: TimeoutError()\n",
            "Exception raised in Job[49]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-4vKkvHgki0O52P13nyzZJ8RE on tokens per min (TPM): Limit 30000, Used 29374, Requested 2004. Please try again in 2.756s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[41]: TimeoutError()\n",
            "Exception raised in Job[47]: TimeoutError()\n",
            "Exception raised in Job[53]: TimeoutError()\n",
            "Exception raised in Job[59]: TimeoutError()\n",
            "Exception raised in Job[65]: TimeoutError()\n",
            "Exception raised in Job[71]: TimeoutError()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.4792, 'faithfulness': 0.5881, 'factual_correctness': 0.3850, 'answer_relevancy': 0.8854, 'context_entity_recall': 0.3871, 'noise_sensitivity_relevant': 0.0000}"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "evaluation_dataset_finetuned = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset_finetuned,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "006bf6f5ebaf400486a6b82610381db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d3fc6edfdab4fe9aff8e805632eadad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ebe8aa7a82124b57bce2d826c1dea0fa",
              "IPY_MODEL_aa11c10b4234452d9430744ab89b59a2",
              "IPY_MODEL_ca05cbcd72cb41d9856804ffb17b26cb"
            ],
            "layout": "IPY_MODEL_cc2a33e9a7ac4c5699346fcbf53b7c95"
          }
        },
        "1016780729b04ba489d488922173ddae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_9198dd0fa8f04aa7b75307aa2d513bfb",
            "style": "IPY_MODEL_59cd26ae53024fbd85431b6683cd119c",
            "value": true
          }
        },
        "13e9bf583f6442d395334947379a280a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18825dc83221412ab602830fc00db71b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6ea48a80c194d128959422368aa0e10",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9f4026c62c60493caa18c014ae414e65",
            "value": "Connecting..."
          }
        },
        "1995b4d98fe044e38f1980d47033ca40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a872283afaa4a33a9bc3f9e57b3650b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "381c780ce17e4662981175400fb8a0f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "408f4dfce21a45cfad36047677ec8658": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a723c6a56e94309b2e3abe63f28aedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_869814ecd49e46f9a33dd53130e6953f",
            "max": 1336413848,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc7d460d3c5a4ac6a9b64929736d6598",
            "value": 1336413848
          }
        },
        "4e2c257232c3472697e03350de43cb30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e7ccd97042c4fb9a201b6dc76762e04": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58699484312f460cb96ac44e3af14aa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_d4acc9a7aca04564bfaefe33de079394"
          }
        },
        "59cd26ae53024fbd85431b6683cd119c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72ec09e2cbbf4788b1561abfcdd0819a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "869814ecd49e46f9a33dd53130e6953f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "878dc96f87dd45f389948a546db33e94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9198dd0fa8f04aa7b75307aa2d513bfb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95160a05de5b402b9bdb0bd2d099cd00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9638a0456e0c41b1b9b9757932f55c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d34dac405fd40139d5dc04eecef55ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4e7ccd97042c4fb9a201b6dc76762e04",
            "style": "IPY_MODEL_72ec09e2cbbf4788b1561abfcdd0819a",
            "tooltip": ""
          }
        },
        "9f4026c62c60493caa18c014ae414e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6ea48a80c194d128959422368aa0e10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa11c10b4234452d9430744ab89b59a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a872283afaa4a33a9bc3f9e57b3650b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe4d1052824c4ed29c38c5311087e650",
            "value": 1
          }
        },
        "acf3991e5d644f468264f561b28b514a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95160a05de5b402b9bdb0bd2d099cd00",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cf376b0ea3544055b8867d7013526502",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "b011a6ccf8e745c6be6f3a17b7d61dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_878dc96f87dd45f389948a546db33e94",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_006bf6f5ebaf400486a6b82610381db0",
            "value": ""
          }
        },
        "bfc4997e3bd94e66bebaa1ffdae1b99e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca05cbcd72cb41d9856804ffb17b26cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e283b1608c4d4266a03c57dc95aabb2e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bfc4997e3bd94e66bebaa1ffdae1b99e",
            "value": "‚Äá0/1‚Äá[00:00&lt;?,‚Äá?example/s]"
          }
        },
        "ca5804644ef345c1b4f670fb7f088fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc1f0bc4a1a74568b9c74a7392a1568f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc2a33e9a7ac4c5699346fcbf53b7c95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "cc7d460d3c5a4ac6a9b64929736d6598": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf376b0ea3544055b8867d7013526502": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d45a7d95e5b14a6f88d5798fec9c40a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e2c257232c3472697e03350de43cb30",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_381c780ce17e4662981175400fb8a0f8",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "d4acc9a7aca04564bfaefe33de079394": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "d6169577ffb341a69eb0175e301b6a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb1e19624fde4d3c8048ce00264d6056",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9638a0456e0c41b1b9b9757932f55c53",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "e283b1608c4d4266a03c57dc95aabb2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5c86e0e33264ed8b6325e44f9421653": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_acf3991e5d644f468264f561b28b514a",
              "IPY_MODEL_4a723c6a56e94309b2e3abe63f28aedc",
              "IPY_MODEL_ef25768d3b0746b48c6d02e9bd151bf9"
            ],
            "layout": "IPY_MODEL_cc1f0bc4a1a74568b9c74a7392a1568f"
          }
        },
        "ebe8aa7a82124b57bce2d826c1dea0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_408f4dfce21a45cfad36047677ec8658",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ca5804644ef345c1b4f670fb7f088fe8",
            "value": "Computing‚Äáwidget‚Äáexamples:‚Äá‚Äá‚Äá0%"
          }
        },
        "ef25768d3b0746b48c6d02e9bd151bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13e9bf583f6442d395334947379a280a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1995b4d98fe044e38f1980d47033ca40",
            "value": "‚Äá1.34G/1.34G‚Äá[01:11&lt;00:00,‚Äá21.0MB/s]"
          }
        },
        "fb1e19624fde4d3c8048ce00264d6056": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe4d1052824c4ed29c38c5311087e650": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
